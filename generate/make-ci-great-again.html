<h2>问题描述</h2><p>其实在这个 <a href="https://github.com/pingcap/tidb/issues/30822">issue</a> 里面，我已经描述过了。</p><p>TiDB 的 CI 不稳定，不稳定的来源一个是环境(次要因素)，一个是测试 case 本身 (主要因素)。<strong>并发的姿势不对，是测试 case 不稳定的根源问题</strong>。</p><p>旧的测试框架下，test suite 之间是并行的，同一个 test suite 内部是各个 test case 串行的</p><p>我们在<a href="https://github.com/pingcap/tidb/issues/30822">重构测试代码</a>，新的代码中，测试是否并发取决于测试 case 在函数开头是否调用 <code>t.Parallel()</code></p><p>不管旧的代码还是新的代码，当前的并发方式，都是一个 package 一个进程，在 package 内部测试并发，<strong>多个 case 跑在同一个进程，代码容易相互影响，导致 CI 不稳定</strong>：</p><ul><li>比如设置过 failpoint 影响其它测试</li><li>比如配置的修改影响到其它测试</li><li>比如代码里面有些全局变量之类的，多个测试在单个进程内跑无法隔离</li></ul><p>还有一种情况，是<strong>并发压力的影响，CI 环境压力不大的时候没事，当环境压力很大之后就不稳定</strong>。</p><p>测试 case 本身写得不好是一方面，而环境对并发度的不可控也对 CI 稳定造成很大困扰，最终后果都是我们花大量的成本在修复 CI。</p><p>修测试的成本是很高的，有些测试在本地环境跑得好好的，但是在 CI 环境就有时候会跪，而复现又困难。受环境影响而挂掉的，这些都特别难排查，加 log 去 CI 上面打印，改一次跑一次的周期特别长。</p><p>还有一种 panic 超时的，就是某个测试里面有 panic 了，或者后台 goroutine 啥的，而测试逻辑又在等待那个结果，永远等不到，测试就卡住了。
最后是超过 10min 才报错退出。</p><p>本来嘛，加单个测试的超时机制，应该能解决掉这个问题。然而加超时机制又引入另外的坑，false negnative... 因为 CI 环境机器压力大，有时候跑得一慢，就误报超时了，然后又不稳定。</p><h2>解决方案</h2><p>初步的解决方案，换一种并发方式，<strong>让每一个测试 case 跑在一个单独的进程里面</strong></p><p>对于旧的代码，测试 case 的执行调用</p><pre><code>go test -test.run '^TestT$' TestXXX
</code></pre><p>对于新的代码，测试 case 的执行调用</p><pre><code>go test -test.run TestXXX
</code></pre><p>这种执行方式就解决单个进程内，测试之间相互影响导致不稳定的问题</p><p>接下来是并发控制部分，<strong>由外部去控制同时跑多少个测试进程</strong></p><p>这个 <a href="https://github.com/pingcap/tidb/pull/30828">PR</a> 中做了一个工具，用一个进程跑一个测试，单个测试内部不再并行; 根据环境 CPU 的数量，决定同时跑多少个进程，可以兼顾到并发，又不让环境压力过大导致 CI 不稳定</p><p>使用方式：</p><pre><code>    make ut // 跑所有测试
    make ut X='list' // 列举有哪些包
    make ut X='list session' // 列举 session 包下有哪些 test case
    make ut X='run planner/core' // 并行执行 planner/core 包下的所有 case
    make ut X='run util/ranger TestTableRange' // 执行 util/ranger 包的单个测试函数
</code></pre><h2>具体实施</h2><p>我列了一个工作计划：</p><ul><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled checked />去掉代码中旧的并行方式 t.Parallel()</li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled checked />提供并行执行 ut 的工具</li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled checked />处理 coverage 等问题</li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled />处理单个测试的超时限制</li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled checked />替换默认的 CI pipeline</li></ul><p>然后记录下这一路克服的障碍。</p><h3>如何有效地并发</h3><p>看了一下当前的 test case 数量，在 tidb 仓库内的单测当前大概是 4700+ 个。如果串行跑太慢了...</p><p>想到的首先是弄几个 worker，有多少个 numa 核，就起多少个 worker，然后把包的所有测试往 worker 发送。但是发现 CPU 压力有点太高了...查了一下发现是 <code>go test -run</code> 的时候，每个进程都假设自己是可以使用全部 GOMAXPROC 的，那么跟 worker 数叠加之后，并发就超载了。解决这个可以通过参数 <code>-test.cpu</code> 指定使用的 CPU 数量。</p><p>按包的粒度的拆分不是特别合理，包与包之间的差异特别大。比如 executor, planner/core 这些是相对比较大的包，而 util/hack 这种包很小。一些要跑挺久，一些瞬间就执行完了。所以改成不管包的大小，把所有包的所有测试函数收集起来，然后用函数粒度去分任务。</p><p>然后是做 shuffle，有些函数可以吃满 CPU，比如 expression 包下面的各种函数，而有些测试 case 里面则的很多的 sleep。做 shuffle 让它们的分布均匀随机一些。</p><p>我又发现 CPU 用不满。我们写的测试，单个测试用例，基本上都是单线程逻辑的，如果给到多个 GOMAXPROC 去执行一个测试，其实只会用到一个 CPU 那么多的资源。最后我干脆就 <code>-test.cpu=1</code> 去跑每个测试，而进程数使用等于 cpu 的数量。</p><h3>处理失败的 case</h3><p>即便当前是每个测试函数跑一个进程的，并发起来的时候，还是有一些可能失败。我发现很典型的是代码里面写死了 /tmp/xxx 文件，
多个测试同时跑着，结果发现自己的 /tmp/xxx 文件没了... 可能被其它的测试用例删除了。</p><p>有也测试是自身的稳定性，即使拿出来单独跟，也有小概率地失败，这种 case 需要针对地修理。</p><h3>numactl 的坑</h3><p>开始时我用了 <code>numactl -- xxx.test -test.run</code> 这样的命令去执行测试，调试时发现有一些神奇的报错发生，有 SIGSEGV 和 SIGTRAP 一类的信号，导致测试进程退出了。我就很纳闷，SIGSEGV 我查不到 core dump，也没看到 Go 那边的退出的栈，设置 GOTRACEBACK 变量也没用...至于  SIGTRAP 就更费解了，啥时候跑 Go 的测试会收到这个信号而退出？</p><p>后来明白了，应该不是测试自己的，而是 numactl 给它发的，而测试中肯定不会处理这些信号，就导致了退出。干脆不要做 numactl 绑核，只是 <code>-test.cpu=1</code></p><h3>生成测试结果报告</h3><p>之前的 CI pipeline 会把测试结果生成一个 JUnitFile 了上传，研究了一下之前的脚本后发现它是用了 <a href="https://github.com/gotestyourself/gotestsum">gotestsum</a> 这个包的功能。&quot;JUnit XML for CI integration, and a summary of the test results&quot;</p><pre><code>gotestsum --junitfile unit-tests.xml
</code></pre><p>这个功能其实挺简单的，我可以把<a href="https://github.com/gotestyourself/gotestsum/blob/661b09182c5e919dfb1d14994f67578f9797164e/internal/junitxml/report.go#L19">文件格式</a>拿到，生成对应的格式就行了。</p><h3>coverage</h3><p>如果是跑单个 package 的测试，<code>go test -coverprofile xxx</code> 可以 coverage 文件。然而新的测试的用户都是每个测试函数单独跑的，虽然也可以生成出来 coverage 文件，但生成的 coverage 就不准确了，只包含了单个测试函数覆盖的。<code>go test -run TestXXX -coverprofile xxx</code></p><p>于是去研究了一下 coverage 文件的格式。其实格式也挺简单的，大概长这种样子：</p><pre><code>mode: set
github.com/pingcap/tidb/server/http_status.go:61.36,63.2 1 1
github.com/pingcap/tidb/server/http_status.go:65.64,72.2 6 0
github.com/pingcap/tidb/server/http_status.go:74.57,75.9 1 0
github.com/pingcap/tidb/server/http_status.go:76.23,76.23 0 0
github.com/pingcap/tidb/server/http_status.go:77.20,77.20 0 0
github.com/pingcap/tidb/server/http_status.go:81.49,83.50 2 1
github.com/pingcap/tidb/server/http_status.go:87.2,90.16 4 1
github.com/pingcap/tidb/server/http_status.go:94.2,96.22 2 1
github.com/pingcap/tidb/server/http_status.go:102.2,102.16 1 1
</code></pre><p>其中第一行是 <code>mode: set</code>，这个 mode 可以是 set, atomic, count ... 由编译参数  -covermode 决定。
剩下的每一行，是 name.go:line.column,line.column numberOfStatements count
源文件是在<a href="https://github.com/golang/go/blob/0104a31b8fbcbe52728a08867b26415d282c35d2/src/cmd/cover/profile.go#L56">这儿</a></p><p>然后我尝试了一下，把包下面的所有测试生成的文件，合并成一个之后，仍然是有效的。</p><p><code>go test -cover -func=xxx.out</code> 可以验证...于是生成 coverage 这个问题就解决了。</p><h3>加速编译</h3><p>刚开始，为了编译每一个包的 test binary，我是遍历整个 package，然后依次 <code>cd pkg; go test -c</code> 的方式弄的。然后发现，这种方式只能形容它是 painfully slow。</p><p>首次的要下载依赖包，编译缓存等，速度最慢可能搞 20min 以上...但是后面的编译，一个包一个包的做法，用不满 CPU。观察发现 Go 编译器的 linking 过程还是单 CPU 的，用不上多核。</p><p>标准的编译方式 <code>go test ./...</code> 这种，它可以一次把多个包编译出来，速度还挺快，编译和运行是一起的。目前 Go 还没有支持并行编译全部的测试包，但不执行。</p><p>google 了一下，找了这里这个<a href="https://github.com/golang/go/issues/15513#issuecomment-773994959">黑科技</a></p><p>用 <code>go test --exec xprog</code>，它会使用 xprog 程序去调用编译好的测试包，我只需要传一个自己的 xprog 程序，就可以控制测试的执行，perfect!</p><p>实际 xprog 接收到的第一个参数，就是编译过程中生成的 xxx.test 文件，一般是在路径 /tmp/go-build3146868117/b1742/xxx.test 下，同目录还有一个 importcfg.link 文件，从里面可以解析出包路径之类的信息。</p>