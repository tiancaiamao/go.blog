<p>接<a href="queueing-delay.md">上篇</a>。</p><p>服务速率 $$&apos;mu$$</p><p>到达速率 $$&apos;lambda$$</p><p>是不是到达速率小于服务速率，就不用排队了？不是的。到达速率是单位时间内，平均到达的请求个数。但是到达是有分布的，并不是说来一个请求，固定多少间隔，再来一个请求这种方式的平均，而是有可能短时间到达了多个，而一段时间没有到达。最终计算期望值，得到的到达速率。关于请求到达的分布的假设是泊松分布。</p><p><strong>排队是一直都在发生的，只是概率上的不同</strong>，这个概率正好是 $$&apos;frac &apos;lambda &apos;mu$$</p><p>到达速率必须小于服务速率，也就是 $$&apos;lambda &lt; &apos;mu $$，否则就表示服务速度跟不上，排队的队列越来越长，系统雪崩。</p><h2>使用率和 idle 率</h2><p>使用率 $$&apos;frac &apos;lambda &apos;mu$$</p><p>idle 率 $$1 - &apos;frac &apos;lambda &apos;mu $$</p><p>如果用概率的角度来解释，当一个请求到达时，当前排队请求数量是0的概率，也就是等于 idle 率。</p><p>换成计算机里面的语言，也就是&quot;说人话&quot;：系统的吞吐的理论上限 $$&apos;mu $$ ，当前的吞吐QPS =  $$&apos;lambda $$。</p><h2>平均队列长度</h2><p>$$  &apos;frac &apos;lambda {&apos;mu - &apos;lambda} $$</p><p>平均队列长度的推导是用队列长度为0、队列长度为1、队列长度为2 ... 的概率得到期望值求得的。</p><p>这个推导其实不难理解。可以拿抽奖来做一个类比，抽中的概率是 P，那么抽一次就抽中的概率是多少？是 P。抽到第二次抽中的概率是多少？(1-P)*P。第三次才抽中的概率是多少 <code>(1-P)^2*P</code>... 第n次中的概率是多少？<code>(1-P)^n * P</code>。那么，平均需要多少次可以抽中？就是算这个期望。</p><p>平均队列长度怎么算的？就跟算平均抽多少次可以抽中一样，只不过 P 和 1-P 就是前面公式中的 idle 率使用率。</p><h2>Little's Law</h2><p>$$ L = &apos;lambda * W $$</p><p>Little's Law 是搞 IT 的人都应该知道的一个公式，非常重要。</p><p>一个朴素的理解方式是：单位时间会进来 $$&apos;lambda$$ 个请求，每个请求处理的耗时是 W，那么当前系统内逗留的请求数是多少，或者说系统容量是多少？也就是两者相乘。在排队论里边，L 就是队列长度。W 是处理时间，也就是排队时间 + 服务时间。</p><h2>平均延迟(响应时间)公式</h2><p>$$ W = &apos;frac 1 {&apos;mu - &apos;lambda} $$</p><p>网上写请求系统逗留时间，应该服从 $$&apos;mu - &apos;lambda $$ 的负指数分布，然后通过负指数分布的期望推导出来 W。
有了 L 和 W 就可以推出它们的关系，从而得到 Little's Law。</p><p>我没法理解这个处理时间服从负指数分布，&quot;因为请求到达服务泊松分布，所以处理时间服从负指数分布&quot;，对我来说太抽象了...</p><p>所以我就直接从 Little's Law 推导出来平均延迟，也就是在这里把 Little's Law 当作公理使用，那么 W 就等于 $$ &apos;frac L &apos;lambda $$</p><p>其中，前面知道了队列长度 L 是 $$&apos;frac &apos;lambda {&apos;mu - &apos;lambda} $$</p><p>那么 W 就是 $$W = &apos;frac 1 {&apos;mu - &apos;lambda} $$</p><h2>平均排队延迟</h2><p>$$&apos;frac &apos;lambda {&apos;mu * ({&apos;mu - &apos;lambda})} $$</p><p>请求延迟等于排队延迟加上处理时间，服务速率是 $$&apos;mu $$，所以纯粹的处理时间是 $$&apos;frac 1 &apos;mu $$</p><p>所以排队延迟就等于总的延迟减去处理时间，也就是</p><p>$$ {&apos;frac 1 {&apos;mu - &apos;lambda}} - {&apos;frac 1 &apos;mu}  $$，也就是</p><p>$$ {&apos;frac 1 {&apos;mu - &apos;lambda}} - {&apos;frac 1 &apos;mu}  == &apos;frac {&apos;mu - ({&apos;mu - &apos;lambda})} {&apos;mu * ({&apos;mu - &apos;lambda})}  == &apos;frac &apos;lambda {&apos;mu * ({&apos;mu - &apos;lambda})} $$</p><h2>平均等待队列长度</h2><p>$$ &apos;frac {&apos;lambda^2} {&apos;mu * ({&apos;mu - &apos;lambda})} $$</p><p>Little's Law 是既适用于整体的 L，也适合于排队的 L 的。也就是</p><p>$$ L_q = &apos;lambda * W_q $$</p><p>所以从平均排队延迟得到平均等待队列长度，只需要乘以一个 $$&apos;lambda $$</p><h2>使用率与响应时间</h2><p>$$ &apos;frac 1 {&apos;mu * (1 - &apos;rho)} $$</p><p>最后这个公式就是我们前一篇提到的，并不是说 CPU 利用低的时候，就不排队 or CPU 没使用满，那么 CPU 就不是瓶颈。</p><p>响应时间公式是</p><p>$$ W = &apos;frac 1 {&apos;mu - &apos;lambda} $$</p><p>其中 $$&apos;frac &apos;lambda &apos;mu $$ 是使用率，如果我们用 $$&apos;rho $$ 表示，那么用 $$&apos;lambda = &apos;mu * &apos;rho $$ 替换掉 W 里面的 $$&apos;lambda $$</p><p>则可以得到使用率与响应时间的关系。</p><p>这个公式是 M/M/1 的公式，也就是服务台个数为1的时候。我看到网上写，推广到 M/M/c 多个服务台c 的场景，使用率跟响应时间大致有这样一个关系：</p><p>$$ W &apos;propto {&apos;frac 1 {1 - &apos;rho^c}} $$</p><p><strong>如果要记两个公式，那么就记 little's law 和延迟公式，剩下的都可以从这两个公式推导。</strong></p><p>最后，看一看具体例子。</p><p>假设服么器一秒钟理论上限处理 300 个请求，也就是 $$ &apos;mu = 300$$，当前的使用率 20% 50% 80% 90% 95% 99% 会发生什么？</p><ul><li>20% 时，响应延迟 4.16667ms, 其中排队延迟 0.8333ms ... 排队延迟占比 20%</li><li>50% 时，响应延迟 6.66667ms, 其中排队延迟 3.3333ms ... 排队延迟占比 50%</li><li>80% 时，响应延迟 16.667ms, 其中排队延迟 13.3333ms ..  排队延迟占比 80%</li><li>90% 时，响应延迟 33.333ms, 其中排队延迟 30ms</li><li>95% 时，响应延迟 66.667ms，其中排队延迟 63.333ms</li><li>99% 时，响应延迟 333.333ms，其中排队延迟 330ms ... 排队延迟占比  99%</li></ul><p>看出来什么了么？使用率高了之后，请求在系统中大部分耗时是在排队！</p>