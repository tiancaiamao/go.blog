<p>今天看到一个很奇怪的指标，它是这样计算稳定性的，每秒完成的请求数，取最大最小平均。</p><p>咋一看，这货不是 QPS 么？但细看发现还不太一样。
计算 QPS 不就是完成的总的请求数，除以总时间么？或者有一个取样的周期，比如每 15s 一次，看这 15s 内完成了多少请求，然后总数除以 15s 求平均。
15s 采样周期的时候，发现 QPS 稳得一批。当这个取样周期到 1s 之后，我观察到服务似乎不够稳定了，波动有达到 10%。然后就有了接下来的思考。</p><p>请求的延迟可以用来度量稳定性，比如 80% 95% 99% 999% 这样的指标。80% 的请求响应时间落在 1ms 以内，99% 的请求响应时间落在 4ms 以内，999% 的请求响应时间落在 16ms 以内。假设请求响应时间服从正态分布，那么给个 avg 给个 80 之类的指标，是可以计算方差以及概率区间的。</p><p>好了，问题来了。</p><p>如果我们从监控上面可以获取到请求的响应延迟分布信息，并且假设请求的响应时间服务正态分布。怎么样计算出，按秒采样的吞吐稳定性？</p><p>例如，响应时间的 80% 是 4ms，99% 是 8ms，并发度为 120，那么每秒的 QPS 在什么样的范围波动，才是合理的？</p><p>从理论上，当采样的频率从 1s 变为 15s 之后，统计出来的 QPS 稳定性是应该变好，还是变差？</p>